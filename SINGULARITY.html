<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Manifesto</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: flex-start; /* Align content to the top */
            min-height: 100vh; /* Ensure body covers the full viewport height */
            background-color: #f0f0f0; /* Light background color for contrast */
        }
        .container {
            width: 60%; /* Control width */
            max-width: 900px; /* Set a max width for larger screens */
            min-width: 300px; /* Prevent it from getting too narrow on small screens */
            background-color: #ffffff;
            padding: 40px;
            margin: 20px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); /* Subtle shadow for depth */
            border-radius: 8px; /* Rounded corners */
            line-height: 1.6; /* Line height for readability */
        }
        h1 {
            text-align: center;
            margin-bottom: 40px;
        }
        h2, h3 {
            margin-top: 40px;
        }
        p, ul {
            margin-bottom: 20px;
        }
        .sidebar {
            font-size: 14px;
            color: #555;
            margin-top: 40px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>staring into the singularity: a reflection</h1>

        <h2>INTRO</h2>
        <p> With my limited knowledge of science, engineering, mathematics and statistics, I want to think 
            about the singularity. Why do I want to think about it? Because I think that thinking about the 
            Singularity, and ways of reaching the singularity, or discussing if the singularity is possible, 
            allows us to dissect ourselves and the way we have constructed reality. The singularity requires 
            the construction of general intelligence, but what is intelligence and what does it tell us about 
            the nature of our reality? These are the ideas that I always want to think about, and in my opinion 
            inform my decision making in various unobvious ways. Anyways now to the singularity. </p>
    
        <p> I'm reading "staring into the singularity" from a blog post by Eliezer Yudkowsky, a self taught American 
            computer scientist and researcher. I found this blog post here:
        </p>
    
        <a href="http://www.fairpoint.net/~jpierce/staring_into_the_singularity.htm">the staring into the singularity</a>



        <h2>THE SHORT VERSION</h2>
        <blockquote>"If computing power doubles every two years, what happens when computers are doing the research?

            <br>

            Computing power doubles every two years. 
            
            <br>

            Computing power doubles every two years of work. 
            
            <br>

            Computing power doubles every two <b>subjective</b> years of work.
            
            <br>

            Two years after computers reach human equivalence, their power doubles again. *One* year later, their speed doubles again.
            
            <br>

            Six months - three months - 1.5 months ... Singularity.
            
            <br>

            It's expected in 2035. (Oops, make that 2025.)"
        </blockquote>
    
            <p>What does “subjective” years of work mean? 

                <br>
                <br>
                I think that when we have adaptive, intelligent systems such as ourselves, that we evolved for survival and moving 
                physically first. Movement, walking, ways to acquire food, ways to digest food. And once we “figured out” a way to 
                do these thing, we have some "free energy" in our system to be used to further make ourselves more efficient and 
                “surviving”. This is purely speculative, I could go into more detail and research to back my hypothesis (on free energy) 
                 but I will leave that for another time. What I'm trying to get at here is that as a system, I believe we have 
                 exhausted our physical efficiency. When we are the most efficient at acquiring food, water and shelter, what's 
                 next? I believe it is our conceptual world. We have complex abstractions of the physical world that inform our 
                 way of life. For example, we all have shelter, so now we can have different aesthetics and organizations of
                  houses that can be looked at from a cultural or social standpoint, these are the abstractions of something physical,
                   that purely exist in a subjective space.
    
            </p>
            <p>What is this subjective space? I think its what we call subjective intelligence. I don't believe there is any concrete 
                theory or definition of intelligence from what I have read so far, but I have developed one. I believe intelligence to be <b>the ability for 
                a system to classify data (inputs) into binary categories without the indication of what the two groups are, but are choosen based on survival</b>.
                I will go more into detail about this at some other time. 
    
            </p>
            <p>I'm also not entirely sure I understand what an entity that is more subjective than me looks like. What will that 
                system that is more intelligent and subjective than me think about the world, or relationships, or anything really. 
                What will it be incentivized to think about?
    
            </p>
            <h2>THE END OF HISTORY</h2>
            <p> Here are graphs based on the writing. The idea is that it should be like exponential decay over time, or something 
                similar to that. Like the time it takes to move to the next stage, is halved. Again its not exactly like that but 
                I like looking at graphs </p>
                <img src="Figure_1.png" alt="Timeline of Key events" width="600" height="400">
                <img src="Figure_2.png" alt="Timeline of Key events but with logs" width="600" height="400">

                <blockquote>"someone will come up with a method of increasing the maximum intelligence on the planet either coding a 
                    true Artificial Intelligence or enhancing human intelligence. An enhanced human would be better at thinking up ways 
                    of enhancing humans; he would have an 'increased capacity for invention'”
                </blockquote>
        
            <p> For some reason this feels wrong. I think humans right now are relatively intelligent but I don't see how we have used our 
                intelligence to make intelligence just for the sake of making more intelligent systems. Something tells me that we should have 
                made it already, because I think we are already pretty intelligent. I think we need some evolutionary incentive to produce more intellegnce.
            </p>
            <p> Also if I use my definition of intelligence (binary classification) then I see that there is an extremely large number of ways 
                to classify the data we get, but there is a finite limit unless we increase the levels of abstraction. For example, take houses. 
                Maybe in our early stages of evolution you either had shelter or no shelter. But now we have various different types of structures. 
                They are ordered and they have a social and even cultural connotations. We are more intelligent than our ancestors during that time. 
                But can we predict the next level of abstraction? I'm not sure about that. Is there an upper limit to the number of abstractions? 
                I'm not sure either. Maybe we won't even be able to understand a machine or human that has more levels of abstractions then us. 
            </p>
            <p> Another question is how can we learn how abstractions are created. I think this might be a consequence of "free energy" in a system 
                but I have no proof of this yet. So maybe the next level of abstractions is the ability for a system to make more efficient better 
                abstractions? For some reason I feel like I'm talking in circles here.
            </p>


            <blockquote>"And what will that doubly enhanced intelligence do? Research methods on triply enhanced humans, or build AI - 
                Artificially Intelligent - assistants or even independent AI researchers who operate at computer speeds. And an AI researcher 
                would be able to reprogram itself, directly, to operate even faster - and better still, smarter. And then our crystal ball 
                explodes, everything we know is out the window, Life As We Know It is over, the "old models break down and new ones must be 
                applied". Hence the phrase: Singularity.”
            </blockquote>

            <blockquote>“Artificial Intelligence - if we can create programs that can match our intelligence, they shall shortly thereafter exceed us 
                in speed if not in ability, because computers are currently increasing in speed by 55% per year. Even a mildly "intelligent" program, 
                with the ability to notice some meaning in text, could greatly increase the rate of scientific progress by making knowledge more accessible.
                 We could bootstrap our way to the Singularity via the relatively mild enhanced humans produced by Algernon's Law. Something completely 
                 unanticipated could occur, such as this decade's invention of the Scanning Tunnelling Probe, which advanced the nanotechnology timetable 
                 by about ten years.”
            </blockquote>
            <p>I am really curious about this because if I create a systems like myself, I don't have confidence it would do anything lol. Probably because I 
                am not smart and don't do much. I'm curious on why people think that if they create a human type system that it will greatly increase the 
                intelligence of the world. Maybe they think that we can control it and incentivize become more intelligent and create a better world, but I 
                have a feeling we wont be able to do that. We still have to remember that we are human, that we exist in this environment the way we are so 
                that we can survive in the world, no other reason. What would be the environment of the intelligent being. I feel like we are starting to 
                create AGI from a level of abstraction that we are currently at, and forget that we are only at this level of abstraction because we have 
                survived the early levels of evolution. It feels like we are starting this stage from the middle, but if we give the system “incentive” we 
                can allow it to evolve similar to us. 

                <br>
                <br>
                This reminds me of the Newtons and his theory of gravitation. When I was in highschool and I learned Newtons laws of gravition, I thought it was 
                so dumb that he didn't think of why, like Einstein. So things fall to the ground, and he quantified that. How fast does it fall, were does it fall, etc.
                But Einstein asked why is it falling in the first place. Similar to that. Darwin came out with the theory of evolution and the idea of natural selection.
                He sort of quantified it or rather observed it. But he never asked the question of why. Why are things changing for the better? What propels a system 
                to change in general and why, sometimes, does the change produce something good. I think there are some theories out there like genetic mutaions, evolutionary algorithms,
                and  randomness. But I think that's the question we should be asking now.
            <br>
            <br>
            I just thought of another idea : the idea of "incentive". I think that biological systems have “incentive” as in they just have survived. 
            Now I think that neural networks have an "incentive" too, and that's back propagation. Maybe that's not what we have in our neural 
            networks in our brain but maybe that is an “incentive”. Just a thought.
                </p>

            <blockquote>“ the Singularity is inevitable.”</blockquote>
            <p>I still have this almost poetic notion that this is what all systems are “programmed” or “incentivized” 
                to reach as a consequence of the physical world. Like if I take a lizard and put him in an ideal environment with predator,
                 prey food etc., that generations from now it will evolved into an intelligent being. I don’t know how long it will take but I think 
                 it will happen at some point.</p>

            <blockquote> “Universe is filled with stable things” </blockquote>
            <br>
            <br>
            <blockquote>“Some terminology, due to Vinge's Hugo-winning A Fire The Deep:
                <br>
                <b>Power</b>  - an entity from beyond the Singularity. 
                <br>
                <br>
                <b>Transcend Transcended, Transcendence</b> - The act of reprogramming oneself to be smarter, reprogramming (with one's new intelligence) to be smarter still, and so on *ad Singularitum.* Also the metaphorical area where the Powers live, or belonging to that area. 
                <br>
                <br>
                <b>Beyond</b> - The grey area between being human and being a Power; the domain inhabited by entities smarter than human, but not possessing the technology to reprogram themselves directly and Transcend.”</blockquote>
                <br>
            <p>I am interested in the terminology here. The way of thinking of a being more intelligent than humans. Why is it thought of as “beyond” us. 
                Would we then be “beyond” compared to our ancestors way back when? I don’t know, but it feels alienating. </p>        
    
    <h2>THE BEYONDNESS OF THE SINGULARITY</h2>
    <blockquote>“ Computer speeds don't double due to some inexorable physical law, but because researchers and engineers find ways to make them faster.
         If some of the researchers and engineers are computers…”</blockquote>

    <p>Again, I think its a characteristic of a system to become better over time. </p>
    
    <blockquote>“What if the quality of thought was enhanced?”</blockquote>
    
    <p>Here he is saying, maybe it’s not just the speed that is enhanced by maybe the quality of the abstractions? </p>

    <blockquote>“ Shouldn't this improve if, say, the total sum of human scientific knowledge is stored in predigested, cognitive, 
        ready-to-think format? Shouldn't this improve with short-term memories capable of holding the whole of human knowledge? A
         human-equivalent AI isn't merely "equivalent" - if Kasparov had had even the smallest, meanest automatic chess-playing program 
         integrated solidly with his intuitions, he would have beat Deep Blue into a pulp. That's The AI Advantage: Simple tasks carried 
         out at blinding speeds and without error, conscious tasks carried out with perfect memory and total self-awareness.”</blockquote>

    <p>If you think about science today, it’s just models of how we think the universe works. There were times where good questions were asked and 
        then there was a paradigm shift in our thinking, in which we asked new questions that our old frameworks simply couldn’t address. 
        Like Einstein asking if there was an object moving in empty space how would we know it was moving, and thus came the idea of relativity.
         Do we get these paradigm shifts because we have all the data and all the ideas? How do we ask questions that are not even in the realm of 
         the framework we have at hand? I do not think that an AI with all the information about physics can generate a new type of physics. 
          But maybe if you give a good AI that is capable of novel thinking all of the understanding of physics, that they can make new frameworks 
          and ideas. </p>

    <blockquote>“Transcended doubling *might* run up against the laws of physics before reaching infinity... but even the laws of physics *as now
         understood* would allow one milligram (more or less) to store and run the entire human race at a million subjective years per 
         second. (Without even using quantum computing.)”</blockquote>
    
    <p>Will the singularity be limited by hardware? Will the singularity be limited by science? Will the singularity be limited by entropy? 
        I don’t really know what that last question means, “limited by entropy”. </p>
    
    
    <blockquote>“Let's take a deep breath and think about that for a moment. One milligram. The <i>entire</i> human race. One <i>million</i> years per second. 
        That means, using only this planet for computing power, it would be possible to support more people than the entire <i>Universe</i> could support 
        if we colonized <i>every single planet.</i> It means that, in a single day, this civilization would have lived over <i>80 billion years,</i> several times 
        older than the age of the Universe to date.”</blockquote>

    <p>Again, I think we are missing something. I think that all adaptable systems have some incentive as a concequence of evoultion. I don't know what that is 
        or how it translates to making AGI but I think it's needed and missing. 
    </p>
    
    <blockquote>“The sequence produced by iterating T(<b>n</b>), S{<b>n</b> } = T(S{<b>n</b>- 1}), is constant for very low values of <b>n</b>. S{0} is defined to be 
        0; a program of length zero produces no output. This corresponds to a Universe empty of intelligence. T(1) = 1. This corresponds to an 
        intelligence not capable of enhancing itself; this corresponds to where we are now. T(2) = 3. Here begins the leap into the Abyss. 
        Once this function increases at all, it immediately tapdances off the brink of the knowable. T(3) = 6? T(6) = 64?”</blockquote>
    
    <p>Sometimes I question intelligence as a concept itself. Maybe intelligence is an abstraction that only exists in our level of system. As
         we get to be more intelligent, its no longer called “intelligence” but something more. If my definition of intelligence 
         from before is the ability for a system to classify data into binary categories, maybe the next level of “intelligence” is something
          completely different. I have another theory on how to order systems based on their level of “intelligence” or difference, but I need 
          to flesh it out better later on. </p>
    <blockquote>“T(64) = vastly more than 1080, the number of atoms in the Universe. T(10^80) is something that only a Transcendent entity is ever 
        going to be able to calculate, and that only if Transcendent entities can create new Universes to supply the necessary computing power. I
         would venture to guess that even T(64) will never be known to any strictly human being.”</blockquote>

    <p>This, I think is a combinatorial problem. Large combinatorial objects (like the weather) are almost impossible to calculate. But I 
        don’t know if calculating a combinatorial object implies intelligence. Think of a calculator, not necessarily intelligent,
         but can do some more calculations than me. But I guess he is about an intelligent system that can make a tool that can do calculations like that, 
         predict  the future etc. </p>

         <br>
    <p>I have this theory about this. We use statistics as a classification of noise (or data). Statistics is just a classification as a consequence of 
        chaos theory of large amounts of data as a result of small changes in the physical worlds. Maybe our concepts of intelligence is just a consequence
         of this as well. I am still struggling with putting together ideas of intelligence and combinatorically large objects, but I think they are 
         connected.</p>
    
    <h2>THE DEFINITION OF SMARTER</h2>
    
    <blockquote>“Smartness is the measure of what you see as obvious, what you can see as obvious in retrospect, what you can invent, and 
        what you can comprehend. To be a bit more precise about it, smartness is the measure of your semantic primitives (what is simple in 
        retrospect), the way in which you manipulate the semantic primitives (what is obvious), the way your semantic primitives can fit together 
        (what you can comprehend), and the way you can manipulate those structures (what you can invent). If you speak complexity theory, the difference
         between obvious and obvious in retrospect, or inventible and comprehensible, is somewhat like the difference between NP and P.”</blockquote>
    
    <p>To go a bit further, “smartness” is a measure of confidence in categorizing physical things into conceptual groups. I can confidently say this is a 
        table or a chair. That is information and pulling out the binary classifications and feeling confident about them is “smartness”. But 
        this is more or less what he is saying with respect to semantic primitives. </p>
    
    <blockquote>“Different humans may have different degrees of the ability to manipulate and structure symbols; different humans may see and invent 
        different things. The great breakthroughs of physics and engineering did not occur because a group of people plodded and plodded and plodded 
        for generations until they found an explanation so complex, a string of ideas so long, that only time could invent it. Relativity and quantum
         physics and buckyballs and object-oriented programming all happened because someone put together a short, simple, elegant semantic 
         structure in a way that nobody had ever thought of before. That is being a little bit smarter; that's where revolutions come from. Not time.
          Not hard work; although hard work was usually necessary, others had worked far harder without result. Raw smartness.”</blockquote>

    <blockquote><b>“someone put together a short, simple, elegant semantic structure in a way that nobody had ever thought of before”</b></blockquote>

    <p>This is interesting, I actually think its more like you get a simple idea and its actionable and applicable right way. I have many many ideas, 
        although I haven’t put much of it into practice I’d like to imagine that maybe one idea might work. But I think it works more like an evolutionary 
        algorithm. Many people have ideas and the only ones that “survive” conceptually is because of how applicable and actionable they are. 
        I actually don’t know what that means yet, but I think that short and simple and elegant is a by product of something more.</p>


    <blockquote>“Why does anything exist at all? Nobody knows. And yet the answer is obvious. The First Cause <i>must</i> be obvious. It has to be obvious to 
        <i>Nothing,</i> present in the absence of anything else, formed from <i>-blank-</i>. What is it that evokes conscious experience, the stuff that souls are 
        made of? We are <i>made</i> of conscious experiences. There is <i>nothing</i> we experience more directly. How does it work? We don't have a clue. <i>Two and 
        a half millennia</i> of trying to solve it and nothing to show for it but "I think therefore I am." The solutions operate outside the semantic 
        primitives and the semantic structures we can use. Our descendants, successors, future selves will figure out the semantic primitives necessary
         and alter themselves to perceive them. The Powers will dissect the Universe and the Reality until they understand why anything exists at all, 
         neurons until they understand qualia. And that will only be the <i>beginning.</i> It won't end there. Why should there be only three hard problems? 
         After all, if not for humans, the Universe would apparently contain only one or two hard problems - how could a non-conscious thinker formulate
          the hard problem of consciousness? Might there be states of existence beyond mere consciousness - transsentience? <i>That's</i> what the Singularity is 
          all about.”</blockquote>
    </div>
</body>
</html>
